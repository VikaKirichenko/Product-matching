{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "load all data and reduce specifications column"
      ],
      "metadata": {
        "id": "L74Or_BOKRJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install umap-learn\n",
        "!pip install hdbscan"
      ],
      "metadata": {
        "id": "7haA45Vsb9qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import ast\n",
        "import re\n"
      ],
      "metadata": {
        "id": "DDyUHsn6KWJX"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "0HW9OZiscANj"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7Kuj0SoKKxJ"
      },
      "outputs": [],
      "source": [
        "df_all = pd.read_csv('/content/drive/MyDrive/diploma/data/all_wb_dns_smartphones.csv')\n",
        "df_all = df_all.drop(columns = ['Unnamed: 0'])\n",
        "df_all.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_2ZPpWALm2p",
        "outputId": "5d42b5a8-4b59-4c3e-eb62-4909ead26e37"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7662, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all.drop_duplicates().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK1Gdw4YLiwI",
        "outputId": "68b03623-a237-49c5-fbd2-5b13a30880a4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7662, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_spec(spec):\n",
        "    all_info = {}\n",
        "    spec = ast.literal_eval(spec)\n",
        "    for block in spec:\n",
        "        for item in spec[block]:\n",
        "            if len(item) > 1:\n",
        "                key = item[0].lower()\n",
        "                val = item[1].lower()\n",
        "                all_info[key] = val\n",
        "\n",
        "    \n",
        "    new_all_info = {}\n",
        "    for info in all_info:\n",
        "        if re.search(r'(модель|объем|предмета|вес|диагональ|страна|камер|емкость аккумулятора|sim|код производителя)', info.lower()):\n",
        "            new_all_info[info] = all_info[info].replace('\"',\"\")\n",
        "\n",
        "    myKeys = list(new_all_info.keys())\n",
        "    myKeys.sort()\n",
        "    sorted_dict = {i: new_all_info[i] for i in myKeys}\n",
        "\n",
        "    return str(sorted_dict)\n",
        "\n",
        "\n",
        "\n",
        "def reduce_spec(df,column):\n",
        "    df[column] = df[column].apply(process_spec)\n",
        "    df.to_csv('data/all_wb_dns_smartphones_reduce_spec.csv')\n",
        "    \n",
        "    return df\n",
        "\n",
        "# reduce_spec(df_all, 'specifications')"
      ],
      "metadata": {
        "id": "4-W4wnZOKyPW"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание двух новых столбцов - код производителя и модель"
      ],
      "metadata": {
        "id": "eaINPpEUOGra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_all = pd.read_csv('data/all_wb_dns_smartphones_reduce_spec.csv')"
      ],
      "metadata": {
        "id": "tTkk0vWtN-3r"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_models(df):\n",
        "    models = []\n",
        "    for ids, row in df_all.iterrows():\n",
        "        row['specifications'] = ast.literal_eval(row['specifications'])\n",
        "        if 'модель' in row['specifications']:\n",
        "            models.append(row['specifications']['модель'])\n",
        "        else:\n",
        "            models.append(\"\")\n",
        "    return models\n",
        "\n",
        "def get_identifiers(df):\n",
        "    identifiers = []\n",
        "    for ids, row in df_all.iterrows():\n",
        "        row['specifications'] = ast.literal_eval(row['specifications'])\n",
        "        if 'код производителя' in row['specifications']:\n",
        "            identifiers.append(re.sub(r'[\\[\\]]',\"\",row['specifications']['код производителя']))\n",
        "        else:\n",
        "            identifiers.append(\"\")\n",
        "    return identifiers\n",
        "\n"
      ],
      "metadata": {
        "id": "p2KUQvehOSv5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO5RZJJ0OOM_",
        "outputId": "5749b7bb-1000-4904-aeb8-efa8d8b297b3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7662, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Генерация пар по столбцу модель"
      ],
      "metadata": {
        "id": "KmKGq5hDPIdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pairs(df, column_name, sequence):\n",
        "    df = df.copy()\n",
        "    pairs = []\n",
        "    for elem in sequence:\n",
        "        pos_prods = df[df[column_name] == elem].reset_index()\n",
        "\n",
        "        idxs = pos_prods['id'].tolist()\n",
        "        titles = pos_prods['title'].tolist()\n",
        "        for i, prod in pos_prods.iterrows():\n",
        "            for idx, title in zip(idxs[i+1:], titles[1:]):\n",
        "                pair = {'id_1':prod['id'], 'title_1': prod['title'],\n",
        "                        'id_2': idx, 'title_2':title, column_name:elem}\n",
        "                pairs.append(pair)\n",
        "\n",
        "    print(len(pairs))\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "S9rrMvwzQU5r"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pairs_by_model(df):\n",
        "    # group by model\n",
        "    groups_model = df.groupby(['model'], as_index = False).count()\n",
        "    # select non empty model name and groups where only 2 products (for reducing number of pairs)\n",
        "    groups_model = groups_model[(groups_model['sku']==2) & (groups_model['model'] != '')].sort_values(by=['sku'], ascending=False)#['identifier'].tolist()[0]\n",
        "\n",
        "    models = groups_model['model'].tolist()\n",
        "    pairs = generate_pairs(df,'model',models)\n",
        "\n",
        "    df_pairs_model = pd.DataFrame(pairs)\n",
        "    df_pairs_model['pair_id'] = range(df_pairs_model.shape[0])\n",
        "    df_pairs_model['match_type'] = -1\n",
        "\n",
        "    df_pairs_model.to_csv('data/pairs_by_model.csv',index = False)\n",
        "\n",
        "    return df_pairs_model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hm-x7aFQPqdG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Генерация пар по столбцу код производителя"
      ],
      "metadata": {
        "id": "XjR1FndvVxWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pairs_by_identifier(df):\n",
        "    groups_ident = df.groupby(['identifier'], as_index = False).count()\n",
        "    groups_ident = groups_ident[(groups_ident['sku']>1) & (groups_ident['identifier'] != '')].sort_values(by=['sku'], ascending=False)#['identifier'].tolist()[0]\n",
        "\n",
        "    identifiers = groups_ident['identifier'].tolist()\n",
        "\n",
        "    pairs = generate_pairs(df,'identifier',identifiers)\n",
        "\n",
        "    df_pairs_ident = pd.DataFrame(pairs)\n",
        "    df_pairs_ident['pair_id'] = range(df_pairs_ident.shape[0])\n",
        "    df_pairs_ident['match_type'] = -1\n",
        "\n",
        "    df_pairs_ident.to_csv('data/pairs_by_identifier.csv',index = False)\n",
        "\n",
        "    return df_pairs_ident"
      ],
      "metadata": {
        "id": "i-uSYekuVzmz"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Генерация пар по модели, объему оперативы, объему встроенной памяти, емкости аккумулятора, диаг экрана"
      ],
      "metadata": {
        "id": "Ia9yhiqpXNXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prods_with_info(df):\n",
        "    prods = []\n",
        "    for ids, row in df.iterrows():\n",
        "        prod = {}\n",
        "        row['specifications'] = ast.literal_eval(row['specifications'])\n",
        "        keys = re.findall(r'(диагональ экрана|емкость аккумулятора|объем встроенной памяти \\(гб\\)|объем оперативной памяти \\(гб\\))',','.join(row['specifications'].keys()))\n",
        "\n",
        "        if len(keys) == 4:\n",
        "            prod['id'] = row['id']\n",
        "            prod['model'] = row['model']\n",
        "            prod['diag'] = re.search(r'\\d+\\.\\d+',row['specifications'].get('диагональ экрана').replace(',',\".\").split(' ')[0].replace(\"''\",\"\"))\n",
        "            if not prod['diag']:\n",
        "                prod['diag'] = \"\"\n",
        "            else:\n",
        "                prod['diag'] = prod['diag'].group()\n",
        "            prod['akk'] = row['specifications'].get('емкость аккумулятора').split(\";\")[0].split(' ')[0]\n",
        "            prod['bim'] = row['specifications'].get('объем встроенной памяти (гб)').replace('гб',\"\").replace('gb',\"\").strip().split(\";\")[0]\n",
        "            prod['ram'] = row['specifications'].get('объем оперативной памяти (гб)').replace('гб',\"\").replace('gb',\"\").strip().split(\";\")[0]\n",
        "            prods.append(prod)\n",
        "    df = pd.DataFrame(prods)\n",
        "    return df"
      ],
      "metadata": {
        "id": "ncczV-_BXMTe"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pairs_by_model_bim_ram(df):\n",
        "    df = get_prods_with_info(df)\n",
        "    agg_func_max_min = {'id': ['min', 'max', 'count']}\n",
        "    groups_ident = df.groupby(['model','diag','akk','bim','ram'], as_index = False, group_keys=True).agg(agg_func_max_min)\n",
        "    groups_ident = groups_ident[(groups_ident['diag']!=\"\") & (groups_ident['model'] != '')& (groups_ident['id']['count'] == 2)]\n",
        "\n",
        "    pairs = groups_ident['id'][['min','max']]\n",
        "    pairs = pairs.rename(columns={\"max\": \"id_1\", \"min\": \"id_2\"})\n",
        "    pairs['match_type'] = -1\n",
        "    pairs['pair_id'] = range(pairs.shape[0])\n",
        "\n",
        "    pairs.to_csv('data/pairs_by_model_diag_akk_bim_ram.csv',index = False)\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2XjVS-eCX9DG"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Генерация пар с помощью кластеризации и близости названия (описания) и цены"
      ],
      "metadata": {
        "id": "ylXdSvsSbG35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('russian')\n",
        "\n",
        "word_tokenizer = nltk.WordPunctTokenizer()\n",
        "\n",
        "def process_data(text):\n",
        "    text = text.lower().replace(\"/\",\" \").replace(\"+\",\" \").replace('смартфон',\"\").replace('гб','gb').strip()\n",
        "    return text\n",
        "    \n",
        "def process_price(price):\n",
        "    price = price.split(\"\\n\")[0]\n",
        "    return price\n",
        "\n",
        "def process_description(desc):\n",
        "    # print(desc)\n",
        "    if desc is not None:\n",
        "        desc = desc.lower().replace('смартфон',\"\").replace('гб','gb').replace(', ',' ')\n",
        "        desc = re.sub(r'[\\-\\:]',\" \",desc)\n",
        "        desc = re.sub(r'[!?\"]',\"\",desc)\n",
        "        desc = re.sub(r'\\s+',\" \",desc)\n",
        "\n",
        "        tokens     = desc.split(\" \") # splits the text into tokens (words)\n",
        "        # еще бы хорошо все это привести в нормальную форму лемматизировать\n",
        "        # remove punct and stop words from tokens\n",
        "        tokens = [word for word in tokens if (word not in string.punctuation and word not in stop_words)]\n",
        "        return \" \".join(tokens)\n",
        "    return \"\"\n",
        "\n",
        "def preprocess_columns(df_all):\n",
        "    df_all['title'] = df_all['title'].apply(process_data)\n",
        "    df_all['brand'] = df_all['brand'].apply(process_data)\n",
        "    df_all['description'] = df_all['description'].fillna(\"\")\n",
        "    df_all['description'] = df_all['description'].apply(process_description)\n",
        "\n",
        "    # replace empty title with first 5 words in description\n",
        "    df_all.loc[df_all['title'] == \"\", ['flag']] = 'nothing'\n",
        "    df_all.loc[df_all['title'] != \"\", ['flag']] = 'full'\n",
        "    df_all['title'].mask(df_all['title'] == '', df_all['description'].apply(lambda x: \" \".join(x.split()[:5])), inplace=True)\n",
        "    # end of replacing\n",
        "\n",
        "    df_all = df_all[df_all['title'] != \"\"]\n",
        "    df_all = df_all.drop_duplicates(subset=['title','brand','price','description'])\n",
        "\n",
        "    return df_all\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsHBek_ubPSg",
        "outputId": "b2e1699d-be99-4ba5-9369-5eba9df234b9"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_pairs(product, df, pairs):\n",
        "    new_pairs = []\n",
        "    for idx,row in df.iterrows():\n",
        "        pair = {\n",
        "            'id_1': product['id'],\n",
        "            'title_1': product['title'],\n",
        "            'id_2': row['id'],\n",
        "            'title_2':row['title']\n",
        "        }\n",
        "        new_pairs.append(pair)\n",
        "    pairs.extend(new_pairs)\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "xmXI0HLLdDky"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pairs_for_product(product, products, all_pairs):\n",
        "    products = products.drop(products[products['id'] == product['id']].index)\n",
        "    # необходимо со всеми товарами в кластере посчитать близость по title, price, (title + первые 5 слов description?)\n",
        "    \n",
        "    titles = products['title'].tolist()\n",
        "    # закодировать titles в sentence_embeddings\n",
        "    main_prod_embeddings = model.encode(product['title'])\n",
        "    row_prod_embeddings = model.encode(titles)\n",
        "\n",
        "    prices = products['price'].tolist()\n",
        "\n",
        "    similarities = cosine_similarity(\n",
        "        [main_prod_embeddings],\n",
        "        row_prod_embeddings \n",
        "    )\n",
        "    price_sim = [1 - abs(float(product['price']) - float(price))/max(float(product['price']),float(price)) for price in prices]\n",
        "\n",
        "    products['price_sim'] = price_sim\n",
        "    products['title_sim'] = similarities[0]\n",
        "    \n",
        "    # отсортировать сначала по title_sim, потом по price_sim\n",
        "    products = products.sort_values(by=['title_sim', 'price_sim'], ascending=False)\n",
        "    # print(products[:10])\n",
        "    pairs = []\n",
        "    # get top 2\n",
        "    top_prod = products.head(2)\n",
        "    pairs = collect_pairs(product, top_prod, pairs)\n",
        "\n",
        "    # get random from middle  1 (in 50% of all)\n",
        "    start = int(products.shape[0] * 0.25)\n",
        "    end =  int(products.shape[0] * 0.75)\n",
        "    middle_random_prod = products[start:end].sample(n=1)\n",
        "    pairs = collect_pairs(product, middle_random_prod, pairs)\n",
        "\n",
        "    # get lowest 2\n",
        "    lowest_prod = products.tail(2)\n",
        "    pairs = collect_pairs(product, lowest_prod, pairs)\n",
        "\n",
        "    # get random 2\n",
        "    random_prod = products.sample(n=2)\n",
        "    pairs = collect_pairs(product, random_prod, pairs)\n",
        "\n",
        "    df_pairs = pd.DataFrame(pairs)\n",
        "    # print(df_pairs)\n",
        "\n",
        "    all_pairs = pd.concat([all_pairs, df_pairs])\n",
        "    all_pairs = all_pairs.drop_duplicates()\n",
        "    return all_pairs"
      ],
      "metadata": {
        "id": "IClUHo7qc4EY"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "import hdbscan\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "\n",
        "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "def cluster_products(df_all):\n",
        "    titles = df_all['title'].tolist()\n",
        "    \n",
        "    sentence_embeddings = model.encode(titles)\n",
        "    # уменьшение размерности\n",
        "    umap_embeddings = umap.UMAP(n_neighbors=15, \n",
        "                            n_components=5, \n",
        "                            metric='cosine').fit_transform(sentence_embeddings)\n",
        "\n",
        "    # кластеризация\n",
        "    cluster = hdbscan.HDBSCAN(min_cluster_size=15,min_samples=1,\n",
        "                          metric='euclidean',                      \n",
        "                          cluster_selection_method='eom').fit(umap_embeddings)\n",
        "    # сохранение топиков\n",
        "    df_all['cluster_id'] = cluster.labels_\n",
        "    docs_per_topic = df_all.groupby(['cluster_id'], as_index = False).agg({'title': ' '.join})\n",
        "    docs_df = df_all[['id','cluster_id','title','brand','price']]\n",
        "\n",
        "    return docs_df\n",
        "\n",
        "def get_random_products_per_cluster(products):\n",
        "    n = 2 if products.shape[0] < 30 else 3 if products.shape[0] < 40 else 4 if products.shape[0] < 50 else 5\n",
        "    # генерация рандомных товаров внутри кластера\n",
        "    random_products = []\n",
        "    # подсчет уникальных названий товаров\n",
        "    n_unique_numbers = len(products['title'].unique())\n",
        "    # если сгенирированный n > чем уникальных значений, то урезаем, иначе будет очень много похожих пар\n",
        "    if n > n_unique_numbers:\n",
        "        n = n_unique_numbers\n",
        "    # print(n,'random products')\n",
        "    count = 0\n",
        "    # до тех пор пока не сгенерировалось нужное кол-во товаров, генерируем\n",
        "    while len(random_products) < n:\n",
        "        count += 1\n",
        "        # выбираем рандомный товар\n",
        "        product = products.sample(n = 1)\n",
        "        product = {\n",
        "            'id':product['id'].tolist()[0],\n",
        "            'title':product['title'].tolist()[0],\n",
        "            'brand':product['brand'].tolist()[0],\n",
        "            'price':product['price'].tolist()[0]\n",
        "        }\n",
        "        # добавляем первый товар\n",
        "        if len(random_products) == 0:\n",
        "            random_products.append(product)\n",
        "            # count += 1\n",
        "            products = products.drop(products[products['id'] == product['id']].index)\n",
        "            continue\n",
        "        # смотрим на близость по названию в случае, если уже есть добавленные товары\n",
        "        sim = 0\n",
        "        prev_prod_embeddings = model.encode(random_products[-1]['title'])\n",
        "        prod_embeddings = model.encode(product['title'])\n",
        "        sim = cosine_similarity([prev_prod_embeddings] ,[prod_embeddings])\n",
        "        # исходя из того, какая итерация, treshold, для того, чтобы даже в кластере, где практчески только одинаковые названия, нашлись близкие\n",
        "        treshold = 0.95 if count < 10 else 0.97\n",
        "        if sim >= treshold:\n",
        "            sim = 1\n",
        "            continue\n",
        "\n",
        "        random_products.append(product)\n",
        "        # убираем из products добавленный товар, чтобы не попался еще раз\n",
        "        products = products.drop(products[products['id'] == product['id']].index)\n",
        "\n",
        "    return random_products\n",
        "\n",
        "\n",
        "def get_pairs(df):\n",
        "    docs_df = cluster_products(df)\n",
        "    n_cat = len(df['cluster_id'].unique())\n",
        "    print(n_cat)\n",
        "    all_pairs = pd.DataFrame(columns = ['id_1','title_1','id_2','title_2'])\n",
        "    for i in range(n_cat):\n",
        "        print(f'category {i}')\n",
        "        products = docs_df[docs_df['cluster_id'] == i]\n",
        "        products_copy = products.copy()\n",
        "        if products.shape[0] < 6 or products.shape[0] > 150:\n",
        "            continue\n",
        "        \n",
        "        random_products = get_random_products_per_cluster(products)\n",
        "\n",
        "        for product in tqdm(random_products):\n",
        "            all_pairs = get_pairs_for_product(product, products, all_pairs)\n",
        "\n",
        "    all_pairs = all_pairs.drop_duplicates(subset=['id_1','title_1','title_2'])\n",
        "    all_pairs.to_csv('data/all_pairs_smartphones2.csv')\n",
        "    return all_pairs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v68GUkxkcJKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def main():\n",
        "df_all = pd.read_csv('/content/drive/MyDrive/diploma/data/all_wb_dns_smartphones.csv')\n",
        "df_all = df_all.drop(columns = ['Unnamed: 0'])\n",
        "# df_all = reduce_spec(df_all, 'specifications')\n",
        "# # df_all = pd.read_csv('data/all_wb_dns_smartphones_reduce_spec.csv')\n",
        "# df_all['model'] = get_models(df_all)\n",
        "# df_all['identifier'] = get_identifiers(df_all)\n",
        "\n",
        "# pairs_by_model = generate_pairs_by_model(df_all)\n",
        "\n",
        "# pairs_by_identifier = generate_pairs_by_identifier(df_all)\n",
        "\n",
        "# pairs_by_model_bim_ram = generate_pairs_by_model_bim_ram(df_all)\n",
        "\n",
        "df_all = preprocess_columns(df_all)\n",
        "df_all = df_all.drop_duplicates(subset=['title','brand','price','description'])\n",
        "all_pairs = get_pairs(df_all)\n"
      ],
      "metadata": {
        "id": "9yU9alueRqb7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}